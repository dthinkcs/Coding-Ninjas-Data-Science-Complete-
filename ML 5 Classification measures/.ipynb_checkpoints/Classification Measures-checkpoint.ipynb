{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Confusion Matrix</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When we build models, it is important to assess how good or bad our model is, and how well it performs on unseen data. Several metrics like accuracy, time taken etc. exist to evaluate model performance. We will see some of the most important and useful ones for the same.</p>\n",
    "<p>What all metrics can we use to evaluate the performance of a classification model? The obvious thing that comes to mind is accuracy over an unseen test set. Accuracy is simply the number of values correctly predicted.</p><p>\n",
    "There is another metric called the confusion matrix, which is a matrix consisting of the number of predicted and actual values for both classes. Confusion matrix is useful in that we can assess how many predictions the model got right, and we understand that the model is performing in this particular way so we now think about how we can further improve our model.</p>\n",
    "<p>There are some terms that one must know regarding confusion matrices.</p>\n",
    "<ol>\n",
    "    <li>True Positives: This is the number of samples predicted positive which were actually positive.</li>\n",
    "    <li>True Negatives: This is the number of samples predicted negative which were actually negative.</li>\n",
    "    <li>False Positives: This is the number of samples predicted positive which were <b>not</b> actually positive.</li>\n",
    "    <li>False Negatives: This is the number of samples predicted negative which were <b>not</b> actually negative.</li>\n",
    "</ol>\n",
    "<p>In the case of multi-class classification, however, the confusion matrix shows the number of samples predicted correctly and wrongly for each class instead of true positives etc.</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_true = [0,0,1,0,1] # dummy label data\n",
    "y_pred = [1,1,0,0,1] # dummy predicted data\n",
    "\n",
    "print(confusion_matrix(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>Classification Measures</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>There are measures other than the confusion matrix which can help achieve better understanding and analysis of our model and its performance. We talk about two particular measures here - precision and recall.</p>\n",
    "<p>Note that precision and recall will be defined per class label, not for the dataset as a whole. Precision defines the percentage of samples with a certain predicted class label actually belonging to that class label. Recall defines the percentage of samples of a certain class which were correctly predicted as belonging to that class.</p>\n",
    "<p>However, how do we choose between precision and recall? Which one is a better metric - precision or recall? Turns out, we can use a better metric which combines both of these - the f1 score. The f1 score is defined as the harmonic mean of precision and recall, and is a far better indicator of model performance than precision and recall (usually).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.33      0.40         3\n",
      "          1       0.33      0.50      0.40         2\n",
      "\n",
      "avg / total       0.43      0.40      0.40         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Tip: Accuracy is not always a good measure of model performance. Accuracy fails when the class labels are highly unbalanced, simply because the accuracy will be high owing to the model predicting a large number of samples as belonging to the majority class label. In such cases, f1 score is a better metric. There are some other metrics like ROC_AUC, which stands for Receiver Operating Characteristic - Area Under Curve. That is, it returns the area under the ROC curve.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
